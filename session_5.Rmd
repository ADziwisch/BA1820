---
title: "Session 5"
author: "Antoine Vernet"
date: ''
output:
  beamer_presentation:
    slide_level: 2
  ioslides_presentation:
    style: style.css
subtitle: Covariance | Correlation | Distributions
---

```{r library, echo = FALSE}
library(ggplot2)
```

## Lesson plan

Today we will cover:

- Covariance
- Correlation
- Some other useful distribution (beyond the normal distribution)
- The central limit theorem
- Bayes theorem
- The Monty Hall problem
- Group projects


## Covariance

The covariance describes how two random variables relate to each other.
It is defined for random variables $X$ and $Y$ as:

$$
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Var}{\operatorname{Var}}
\Cov(X, Y) = (X - \mu_X)(Y - \mu_Y)
$$

It is denoted $\sigma_{XY}$ or . When $\sigma_{XY} \ge 0$, then on average, when $X$ is above (below) its mean, $Y$ is also above (below) its mean. When $\sigma_{XY} \le 0$, then on average when $X$ is above (below) its mean, $Y$ is below (above) its mean. 

## Some properties of covariance

Let $X$ and $Y$ be two random variables, and $a_1$, $a_2$ and $b_1$ and $b_2$ be constants,

$$
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\Var}{\operatorname{Var}}
\begin{aligned}
\Cov(a_1, X) &= 0 \\
\Cov(X, Y) &= \Cov(Y, X) \\
\Cov(X, X) &= \Var(X) \\
\Cov(a_1 X, a_2 Y) &= a_1 a_2 \Cov(X, Y) \\
\Cov(X + b_1, Y + b_2) &= \Cov(X, Y)
\end{aligned}
$$

## Example of covariance

```{r, out.width = '40%', fig.retina = NULL, fig.align = 'center'}
library(ggplot2); set.seed(32)
x <- rnorm(100, mean = 1, sd = 2)
y <- 2 * x + rnorm(100, mean = 1, sd = 2)
dt <- data.frame(x = x, y = y)
ggplot(data = dt, aes(x = x, y = y)) + geom_point()
cov(x, y)
```

## Example of covariance (2)

```{r, out.width = '40%', fig.retina = NULL, fig.align = 'center'}
x <- 1:100
y <- rnorm(100, mean = 5, sd = 10)
dt <- data.frame(x = x, y = y)
ggplot(data = dt, aes(x = x, y = y)) + geom_point()
cov(x, y)
```

## Correlation

Covariance gives you information about whether two variables vary in the same or opposite direction, but does not inform you about how much they do.

The correlation coefficient, as defined by Pearson, is denoted $\rho(X, Y)$ and is equal to the covariance of X and Y divided by the product of their standard deviations:

$$
\rho(X,Y) = \frac{COV(X, Y)}{\sigma_{X} \sigma_{Y}}
$$

## Example of correlation

```{r, out.width = '40%', fig.retina = NULL, fig.align = 'center'}
x <- rnorm(100, mean = 5, sd = 10)
y <- rnorm(100, mean = 5, sd = 10)
dt <- data.frame(x = x, y = y)
ggplot(data = dt, aes(x = x, y = y)) + geom_point()

cor(x, y)
```

## Correlation test

```{r}
cor.test(x, y)
```

## Correlation examples (2)

```{r, out.width = '40%', fig.retina = NULL, fig.align = 'center'}
x <- 1:100
y <- rnorm(100, mean = 5, sd = 10)
dt <- data.frame(x = x, y = y)
ggplot(data = dt, aes(x = x, y = y)) + geom_point()
cor(x, y)
```

## Correlation examples (3)

```{r, echo = FALSE, out.width = '90%', fig.retina = NULL, fig.align = 'center'}
knitr::include_graphics('img/Correlation_examples.png')
```

## Anscombe's Quartet in R

```{r echo = TRUE}
summary(anscombe)

```

## Anscombe's Quartet in R (2)

```{r echo = TRUE}
apply(anscombe, 2, mean)
apply(anscombe, 2, sd)
```


## Anscombe's Quartet

```{r, echo = FALSE, out.width = '70%', fig.retina = NULL, fig.align = 'center'}
knitr::include_graphics("img/Anscombe's_quartet.png")
```


## Conditional expectation

The conditional expectation of a random variable, X, is another random variable equal to the expectation of X for every condition.

$$
E(X | H) = \frac{\sum_{\omega \in H} X(\omega)}{|H|}
$$

## Example

```{r echo = TRUE}
gender <- c(rep("Female", 50), rep("Male", 50))
height <- vector(mode = "numeric", length = 100)
for (i in 1:100){
  if(gender[i] == "Female"){
      height[i] <- 161.3 + rnorm(1, 0, 6)
  }else{
    height[i] <- 175 + rnorm(1, 0, 7)
  }
}
data <- data.frame(gender = gender, height = height)

```

## Example

```{r echo = TRUE}
mean(data[, "height"])

mean(data[data[, "gender"] == "Female", "height"])
```

## Example

```{r echo = TRUE, fig.align = "center", out.width = "70%"}
ggplot(data = data, aes(x = height)) + geom_density() + 
  geom_density(aes(x = height, color = gender))
```


## Conditional variance

The conditional variance of a random variable, X, is another random variable equal to the expectation of X for every condition.

```{r echo = TRUE}
sd(data[, "height"])
sd(data[data[, "gender"] == "Female", "height"])
```




# Useful Distributions

## The normal distribution

The normal distribution is defined by its mean and standard deviation.

- It is an important distribution because of the central limit theorem (more on this later)

```{r, echo = FALSE, out.width = '70%', fig.retina = NULL, fig.align = 'center'}
knitr::include_graphics("img/Normal_Distribution_PDF.png")
```

## Chi-square

The chi-squared distribution with $k$-degrees of freedom is the sum of square of $k$ independent normal random variables. 
If $X_1, X_2, ..., X_i$ are normal random variables:
$$
Q = \sum_{i=1}^k X_i^2
$$

The chi-squared distribution is usually denoted $\chi^2(k)$ or $\chi^2_k$.

Because of its relation to the normal distribution it is often used in hypothesis testing.

## Chi-square

```{r, echo = TRUE, fig.align = "center", out.width = "50%", fig.retina = NULL}
chisq1 <- rchisq(1000, df = 2)
chisq2 <- rnorm(1000, 0, 1) ^ 2 + 
  rnorm(1000, 0, 1) ^ 2
data <- data.frame(x = 1:1000, chisq1 = chisq1, 
                   chisq2 = chisq2)
ggplot(data = data, aes(x= chisq1)) + geom_density() + 
  geom_density(aes(x = chisq2), colour = "red")

```

## T-distribution (Student)

For a given sample, of size $n$, drawn from a normally distributed random variable, Student's t-distribution is the distribution of the location of the true mean of the random variable relative to the sample mean and standard deviation (and multiplied by $\sqrt{n}$).

Because of this, Student's t-distribution can be used to produce estimations that the population mean falls within a specific range.

By construction, as $n$, the sample size, grows, Student's t-distribution approximates a normal distribution.

## T-distribution

```{r, eval = FALSE, echo = TRUE, fig.align = "center", out.width = "50%", fig.retina = NULL}
ggplot(data.frame(x = c(-3, 3)), aes(x)) + 
  stat_function(fun = dt, args = list(df = 5)) +
  stat_function(fun = dt, args = list(df = 1), 
                colour = "red") +
  stat_function(fun = dt, args = list(df = 2), 
                colour = "blue") +
  labs(y = "Density")
```

## F-distribution

A random variable $X$ follow an F-distribution if it arises from the ratio of two scaled $\chi^2$ random variables:

$$X = \frac{U_1 / d_1}{U_2 / d_2}$$ where $U_1$ and $U_2$ are $\chi^2$ distributed and $d_1$ and $d_2$ are the respective degrees of freedom of $d_1$ and $d_2$.

It is often used to estimate whether two variances are equal.


## Central limit theorem

The central limit theorem states that a sample taken from a sufficiently large set of independent random variables that are identically distributed will be normally distributed.

If {X_1, ..., X_n} is a random sample of size $n$ of identically distributed random variables, with mean $\mu$ and variance $\sigma^2$, then the sample average of this variables is $S_n := (X_1 + ... + X_n) / n$ if $n \to \infty$, then $S_n \to \mu$

The CLT allows us to approximate certain processes using the normal distribution.


## Bayes theorem

Bayes' theorem allows us to calculate probabilities of a specific event conditional on other event. It states the following:

$$
P(A\mid B) = \frac{P(B \mid A) \, P(A)}{P(B)}
$$

It as an several interpretation, including one which gave rise to a branch of statistics called _Bayesian statistics_.

## Monty Hall problem

The Monty Hall problem is an application of Bayes' theorem.

The problem is the following:

- Your are playing in a TV game show. There are 3 doors. 
- Behind two of those doors there is a goat, behing the third door there is a prize.
- The host asks you to pick a door. 
- Once you have done so, the host choose one of the remaining doors and opens it to reveal a goat. 
- Before opening your door, he offers you the chance to switch door.

Should you switch?


## Group project

With you syndicate group, you will complete a short report on a dataset.
The report is due on Sunday the 16^th^ of October.
The report will cover the following:

- Description of the dataset (dimensions, content of each of the variables).
- Descriptive statistics (univariate descriptive statistics and bivariate).
- Correlation analyses
- Regression
- _Above all_: you should tell me something interesting about what is going on in the data.

## Deliverable

- The report is a .Rmd file that compiles either to pdf, html or word.
- I also want access to the git or bitbucket repository you guys used during collaboration.

## Dataset

I have released a movie dataset, which you can use. You can also identify and use datasets from the following sources:

- The [HBS dataverse](https://dataverse.harvard.edu/dataverse/hbs?q=&types=dataverses%3Adatasets&sort=dateSort&order=desc&page=3)
- The [American Journal of Political Science dataverse](https://dataverse.harvard.edu/dataverse/ajps)
- The [Abdul Latif Jameel Poverty Action Research Lab](https://dataverse.harvard.edu/dataverse/jpal)
- The [Pew Research Center](http://www.pewresearch.org/data/download-datasets/)

If you have another dataset that you would like to use, come and ask me.

# {.flexbox .vcenter}

![](img/fin.png)\
