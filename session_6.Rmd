---
title: "Session 6"
author: "Antoine Vernet"
date: ''
output:
  beamer_presentation:
    slide_level: 2
  ioslides_presentation:
    style: style.css
subtitle: Sampling | Hypothesis Testing
---

```{r library, echo = FALSE}
library(ggplot2)
```

## Lesson plan


Today we will cover:

- Quiz
- Solution of the exercises for week 2
- Population and parameters
- Sampling
- Estimators
- Hypothesis testing

## Inference

In statistical inference, we are trying to:

- Learn something about a __population__ given a __sample__ of this population
- We want to __estimate__ properties of this population
- Or we want to __test hypotheses__ about this population

## Population

A population is well defined group of subjects. For example:

- individuals
- firms
- countries

## Sampling

If $Y$ is a random variable with probability density function $f(y; \theta)$, where $f$ depends on the parameter $\theta$. We assume that we know the pdf of $Y$ given that we know $\theta$. For this reason, we are interested to learn about the value of $\theta$

## Parameters 

If $f(x) = ax + b$, we have seen that:

- $x$ is a variable
- $a$ and $b$ are parameters

If $f(x)$ is a normally distributed random variable, it is characterised by two parameters:

- $\mu$, its expectation
- $\sigma^2$, its variance

## Random sampling

The simpler sampling scheme is random sampling.
If we sample ${Y_1, Y_2,..., Y_n}$ from the density $f(y; \theta)$ randomly, the $Y_i$ are independent identically distributed random variables from $f(y; \theta)$

We obtain a sample, ${y_1, ..., y_n}$, which is the data we work with. Each sample is going to be different, but they are drawn from the same underlying probability density function.

We routinely simplify by considering that ${Y_1, ..., Y_n}$ is a random sample from the normal population, $\mathcal{N}(\mu, \sigma^2)$.

In inference we often want to know about $\mu$.



## Estimator

If we draw a random sample ${Y_1, ..., Y_n}$ from a population whose distribution depends on parameter $\theta$, an estimator of $\theta$ is a rule that assigns each possible outcome of the sample a value of $\theta$. The rule is specified before the sample is collected.

Concretely, what could we use as an estimator of the mean $\mu$ of the underlying population from which we sampled the sample ${Y_1, ..., Y_n}$?

> The mean of the random sample is a good candidate


## Estimator | Definition

An estimator of $\theta$ can be expressed as:

$$
W = h(Y_1, ..., Y_n)
$$
Where $h$ is a function of the random variables $\{Y_1, ..., Y_n\}$
When a sample is drawn, say ${y_1, ..., y_n}$, computing the value of $h$ for that sample is an _estimate_ of $\theta$.

This is sometimes refered to as a _point estimator_ ($W$) and a point estimate ($w$), to differentiate them from interval estimators.


## Unbiasedness

An estimator for $\theta$ is unbiased if, for all values of $\theta$:

$$
E(W) = \theta
$$

If an estimator is biased, its bias is equal to:

$$
Bias(W) \equiv E(W) - \theta
$$

## Example

Let the population follow the $\mathcal{N}(0, 1)$ distribution.
We define $\overline{Y}$ as the sample mean. We now define to estimators of $\mu$:

- the sample mean ($W_1$)
- the sample mean plus unity ($W_2$)

## Example (2)

```{r, echo = TRUE, out.width = "40%", fig.align = "center"}
w_1 <- rnorm(1000, 0, 1)
w_2 <- rnorm(1000, 0 , 1) + 1

bias <- data.frame(w_1 = w_1, w_2 = w_2)

ggplot(data = bias) + geom_density(aes(x = w_1)) +
  geom_density(aes(x = w_2))


```


## Bias

Bias is a desirable properties, however:

- Some unbiased estimators are poor (e.g. using the expectation of the first element of a sample for estimating $\mu$)
- Some biased estimators are pretty good

## Additional issues

Having an estimator for the mean that is unbiased is a start but for hypothesis testing, we need to have estimators for other properties of the sample distribution.

For example, we need to estimate its variance.

## Why estimate variance

```{r, out.width = "40%", fig.align = "center"}
w_1 <- rnorm(1000, 0, 1); w_2 <- rnorm(1000, 0, 2)
variance <- data.frame(w_1 = w_1, w_2 = w_2)
ggplot(data = variance) + geom_density(aes(x = w_1)) +
  geom_density(aes(x = w_2))
```

## Sampling variance

The variance of the sample average for estimating the mean $\mu$ from a population is:

$$
Var(\overline{Y}) = \sigma^2/n
$$
In other words, the variance of our estimator for the average is the variance of the population divided by the sample size.

An important property of the sampling variance is that it tends towards $0$ when the sample size, $n$, grows.

## Sampling variance

```{r, echo = TRUE}
y_1 <- y_2 <- vector(mode= "numeric", length = 20)
for (i in 1:20){
  y_1[i] <- rnorm(1, 2, 1)
  y_2[i] <- mean(rnorm(10, 2, 1))
}
```

## Sampling variance

```{r, echo = TRUE}
summary(y_1)
summary(y_2)
```
## Sampling variance

```{r, echo = TRUE}
var(y_1)
var(y_2)
```

## Efficiency

Generally speaking, efficiency is an approach to compare estimators.

When two estimators are unbiased, we can compare the variance of the two estimators, the most efficient estimator is the one with the smallest variance.

However, that does not work when some or all of the estimators might be biased.


## Efficiency (2)

In this case, we can use the __mean squared error__ (MSE) of the estimators.

For W, an estimator of $\theta$:
$$
MSE(W) = E[(W - \theta)^2]
$$
This measures how far on average the estimator is away from $\theta$

In fact, we can show that $MSE(W) = Var(W) + [Bias(W)]^2$

## Properties of estimators in large sample 

One way to differentiate between estimators is to study how their properties evolve as the sample size grows. Those are called the _asymptotic_ or large sample properties of estimators.
What constitute a large sample, however, depends on the underlying population distribution.


## Consistency

$W_n$ is an estimator of $\theta$, based on a sample $\{y_1, ..., y_n\}$ of size $n$.
$W_n$ is a consistent estimator of $\theta$ if for every $\varepsilon > 0$:

$$
P(|W_n - \theta| > \varepsilon) \to 0 \text{ as } n \to \infty
$$
In other word, the probability that $W_n$ would be far from $\theta$ converges towards $0$ as the sample size grows.

## Probability limits

Another way to express the previous statement is to say that $\theta$ is the probability limit of $W_n$ : $p \, \text{-lim}(W_n) = \theta$  

## Properties of limits (1)

$\theta$ is a parameter and so is $\gamma = g(\theta)$ (with $g$ a continuous function). Then if $p \, \text{-lim}(W_n) = \theta$ and $G_n$ is an estimator of $g(W_n)$:
$$
p \, \text{-lim}(G_n) = \gamma
$$

which can also be expressed as:

$$
p \, \text{-lim} g(W_n) = g(p \, \text{-lim} (W_n))
$$

## Properties of limits (2)

If $p \, \text{-lim}(T_n) = \alpha$ and $p \, \text{-lim}(U_n) = \beta$, then

$$
\begin{aligned}
&p \, \text{-lim}(T_n + U_n) = \alpha + \beta \\
&p \, \text{-lim}(T_n  U_n) = \alpha  \beta \\
&p \, \text{-lim}(T_n / U_n) = \alpha / \beta
\end{aligned}
$$
The last property implies $\beta \neq 0$

## Asymptotic normality

Asymptotic normality is concerned with the shape of the distribution of the estimator as the sample size grows. An estimator is asymptotically normal if for large sample size its distribution approximate a normal distribution.

The CLT is useful here because it states that for any random sample $\{Y_1, ..., Y_n\}$ with mean $\mu$ and variance $\sigma^2$:

$$
Z_n = \frac{\overline{Y_n} - \mu}{\sigma/\sqrt{n}}
$$
so regardless of the population distribution of $Y$, $Z_n$ has mean $0$ and variance $1$. 

# Parameter estimation

## Approaches to parameter estimation

It would be convenient if there was some general approach to create estimators that have properties such as unbiasedness, consistency and efficiency.

We will cover 3 of those methods briefly:

- method of moments
- maximum likelihood
- least square


## Method of Moments

If we can show that $\theta$ is related to the population mean $\mu$ with $\theta = g(\mu)$.

We know that the sample average, $\overline{Y}$ is an unbiased and consistent estimator of $\mu$ so we can replace one by the other and $g(\overline{Y})$ is a consistent estimator of $g(\mu)$. If $g(\mu)$ is a linear function of $\mu$ then $g(\overline{Y})$ is also unbiased.

## Maximum likelihood

$\{Y_1, ..., Y_n\}$ is a random sample from the population distribution $f(y; \theta)$. The joint distribution of $\{Y_1, ..., Y_n\}$ is the product of the densities $f(y_1; \theta) f(y_2; \theta) ... f(y_n; \theta)$, the likelihood function is:
$$
L(\theta; Y_1, ..., Y_n) = f(Y_1; \theta) f(Y_2; \theta) ... f(Y_n; \theta)
$$
Then the maximum likelihood estimator of $\theta$ is the value of $\theta$ that maximises the likelihood function.


## Least squares

The least square method minimizes the sum of squared deviations:

$$
\sum_{i = 1}^n (Y_i - m)^2
$$
The sample mean, $\overline{Y}$ is a least square estimator of $\mu$. In this specific case, it is also the method of moment estimator and the maximum likelihood estimator of $\mu$ (because the underlying population distribution is normal).


# Hypothesis testing

## Example

We talked about Pok√©mon Go in previous lectures.

We looked at this plot:

```{r, echo = FALSE, out.width = "45%", fig.align = "center"}
pok_lab <- read.csv("./data/Pokemon_Go.csv", nrows = 1)
pok <- read.csv("./data/Pokemon_Go.csv", skip = 2, header = FALSE)
names(pok) <- names(pok_lab)
ggplot(data = pok, aes(x = Q1, fill = Q5)) + 
  geom_bar(position = "dodge") +  scale_x_discrete(labels = c(
"Yes, I have heard of it but I have not used it although it is available in my country" = 
                                       "Yes/Available/Not used it", 
"Yes, I have heard of it but I have not used it because it is not available in my country" =
                                     "Yes/Not available", 
                                     "Yes, I have installed this game" = 
                                       "I have used it")) +
        labs(x = "", y = "Count", title = "Having heard of Pok√©mon Go by gender?")
```

But is the difference we observe more than just an effect of the distribution of male and female in the class and random variation?

## Example (2)

Hypothesis testing is a framework to answer the question on the previous slide

We can define $H_0$, the proportion of female and male in the class who use Pok√©mon Go is the same.

Our test statistic would be the observed proportion of female and male who play Pok√©mon Go. 

The proportion of female using Pok√©mon Go is `r round(nrow(pok[pok[, "Q5"] == "Female" & pok[, "Q1"] == "Yes, I have installed this game", ]) / nrow(pok[pok[, "Q5"] == "Female", ]) * 100, digits = 2)` and the proportion of male who play Pok√©mon Go is `r round(nrow(pok[pok[, "Q5"] == "Male" & pok[, "Q1"] == "Yes, I have installed this game", ]) / nrow(pok[pok[, "Q5"] == "Male", ]) * 100, digits = 2)`.

We are asking: can we estimate the probability of getting such a result if in the population, the proportion of male and female who play the game is the same?


## Proportion test

Let's simplify the problem slightly:

```{r}
pok[, "UsedPok"] <- ifelse(pok[, "Q1"] == 
             "Yes, I have installed this game", 
                           "Yes", "No")
```

```{r, echo = FALSE, results = "markup"}
table(pok[, "Q5"], pok[, "UsedPok"])
```

## Proportion test (2)

```{r}
prop.test(table(pok[, "Q5"], pok[, "UsedPok"]), 
          alternative = "greater")
```

## Alternative test

```{r}
chisq.test(pok[, "Q5"], pok[, "UsedPok"])
```


## Steps in Hypothesis testing 

- define a _Null Hypothesis_ $H_0$ and an alternative hypothesis $H_1$
    + $H_0$ always test that there is no significant difference
- Determine the test statistic
- Decide on a _significance level $\alpha$_
    + This is the probability of rejecting $H_0$ when it is true
    + Typical value for $\alpha$ are 0.1%, 1%, 5%, 10%
- Compute the likelihood of observing the test statistic under $H_0$, then determine if:
    + Results are likely to be due to chance ($H_0$ is true)
    + Results are unlikely to be due to chance ($H_0$ is false)

## Tests

Many tests exist to test various hypotheses, the most common include:

- Tests of means (t-test)
- Tests of variance (F test)
- Tests for proportion (z-test)
- Test for count data ($\chi^2$ test)

Today, we will limit ourselves to tests of means.

## Tests on means

Probably the most common statistical test for hypothesis testing is the student t-test.

- This test assume that the population distribution is $\mathcal{N}(\mu, \sigma^2)$

- The null hypothesis is $\mu = \mu_0$ for some value of $\mu_0$

- The test statistic is $t = \frac{\overline{Y} - \mu_0}{s / \sqrt{n}}$

- Under the null hypothesis, $t \sim T_{n-1}$ where $T_{n-1}$ is the student $t$ distribution with $n-1$ degrees of freedom.

## Example

Let's think about what $x$ represents.

```{r, echo = FALSE, out.width = '70%', fig.retina = NULL, fig.align = 'center'}
knitr::include_graphics("./img/DisNormal06.png")
```


## One-sample t-test

When $n$ is large (> 120)

   + the T distribution approximates the standard Normal distribution $\mathcal{N}(0, 1)$
   + the test statistic is approximately normal, even if the parent population is not

## One-sample t-test in R

Let's build a dataset of the average heights of male
```{r, echo = TRUE}
set.seed(6987)
height <- data.frame(id = 1:100, 
                     height = rnorm(100, 175, 7))

mean(height[, "height"])
```

## Let's try a t-test

```{r, echo = TRUE}
t.test(height[, "height"], mu = 175)
```


## Let's play around with it

```{r, echo = TRUE}
t.test(height[, "height"], mu = 174)
```


## Let's play around with it

```{r, echo = TRUE}
t.test(height[, "height"], mu = 177)
```


# {.flexbox .vcenter}

![](img/fin.png)\
